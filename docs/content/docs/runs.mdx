---
title: Runs & Episodes
description: Understanding the Run and Episode lifecycle in HumaLab
---

Runs and Episodes are the core abstractions for organizing your validation experiments in HumaLab.

## Runs

A **Run** represents a complete validation experiment. It contains:
- A scenario definition
- Multiple episodes (individual executions)
- Metrics and statistics
- Metadata (name, description, tags)

### Creating a Run

```python
import humalab as hl

# Initialize HumaLab
hl.init(api_key="your_api_key")

# Create a scenario
scenario = hl.scenarios.Scenario()
scenario.init(scenario={"param": "${uniform(0, 10)}"})

# Create a run
run = hl.Run(
    scenario=scenario,
    project="my_project",
    name="my_run",
    description="Testing parameter variations",
    tags=["experiment", "v1"]
)
```

### Run Parameters

- `scenario` (required): The Scenario instance defining the parameter space
- `project`: Project name for organization (default: "default")
- `name`: Human-readable name for the run
- `description`: Detailed description of the run
- `id`: Custom run ID (auto-generated if not provided)
- `tags`: List of tags for categorization
- `base_url`: Custom API endpoint (optional)
- `api_key`: API key for authentication (optional, can use `hl.init()`)
- `timeout`: Request timeout in seconds (optional)

### Run Properties

```python
# Access run metadata
print(run.id)           # Unique run identifier
print(run.name)         # Run name
print(run.description)  # Run description
print(run.tags)         # List of tags
print(run.project)      # Project name
print(run.scenario)     # Associated scenario
```

## Episodes

An **Episode** represents a single execution with a specific set of parameter values sampled from the scenario distributions.

### Creating Episodes

```python
# Create an episode within a run
with run.create_episode() as episode:

# Optionally specify an episode ID
episode = run.create_episode(episode_id="custom_episode_id")
```

### Episode Lifecycle

Episodes have a clear lifecycle with status tracking:

```python
from humalab.humalab_api_client import EpisodeStatus

# Episode is created
with run.create_episode() as episode:

    # Your validation logic
    try:
        # Execute your task
        result = my_validation_function(episode.config)

        # Log metrics
        episode.log({"score": result.score})

        # Successfully complete
        episode.finish(status=EpisodeStatus.FINISHED)

    except Exception as e:
        # Handle errors
        episode.finish(status=EpisodeStatus.ERRORED, err_msg=str(e))
```

### Episode Statuses

- `FINISHED`: Episode completed successfully
- `ERRORED`: Episode encountered an error
- `CANCELED`: Episode was canceled
- `TIMEOUT`: Episode exceeded time limit

### Accessing Episode Data

```python
# Get the episode's unique identifier
print(episode.episode_id)

# Get the resolved scenario configuration
print(episode.config)

# Get scenario values sampled for this episode
print(episode.episode_vals)

# Check if episode is finished
print(episode.is_finished)

# Get episode status
print(episode.status)
```

## Context Manager Pattern

The recommended way to use Runs is with Python's context manager:

```python
with run:
    for i in range(100):
        with run.create_episode() as episode:
            # Your validation code
            result = validate(episode.scenario)
            episode.log({"metric": result})

        # Episode automatically finishes when exiting context

# Run automatically finishes when exiting context
```

This ensures proper cleanup even if errors occur.

## Multiple Episodes

Run multiple episodes to explore your scenario's parameter space:

```python
with run:
    for i in range(100):
        with run.create_episode() as episode:
            # Each episode gets different parameter values
            # Access directly via episode attributes

            # Run your validation
            result = my_validation(episode)

            # Log episode-specific data
            episode.log({
                "success": result.success,
                "score": result.score
            })
```

## Run-Level Logging

Log data at the run level (across all episodes):

```python
# Create and add metrics at run level
metric = hl.metrics.Metrics()
run.add_metric("overall_score", metric)

# Log data to run-level metrics
for i in range(100):
    with run.create_episode() as episode:
        result = validate(episode)

        # Log to run-level metric
        run.log({"overall_score": result.score})
```

## Finishing Runs

Always finish your runs to ensure data is uploaded:

```python
from humalab.humalab_api_client import RunStatus

# Successful completion (default)
run.finish()

# Or specify status explicitly
run.finish(status=RunStatus.FINISHED)

# Mark as errored with message
run.finish(status=RunStatus.ERRORED, err_msg="Something went wrong")

# Mark as canceled
run.finish(status=RunStatus.CANCELED)
```

### Run Statuses

- `FINISHED`: Run completed successfully
- `ERRORED`: Run encountered an error
- `CANCELED`: Run was canceled

## Code Artifacts

Log code artifacts to track versions:

```python
# Log code content
with open("agent.py", "r") as f:
    code = f.read()

run.log_code(key="agent_code", code_content=code)
```

## Best Practices

1. **Always Use Context Managers**: Ensures proper cleanup
   ```python
   with run:
       # Your code
   ```

2. **Create Episodes in Batches**: For large-scale experiments
   ```python
   with run:
       for batch in range(10):
           for i in range(100):
               with run.create_episode() as episode:
               # Process episode
   ```

3. **Handle Errors Gracefully**: Always finish episodes even on errors
   ```python
   try:
       result = validate(config)
       episode.finish(EpisodeStatus.FINISHED)
   except Exception as e:
       episode.finish(EpisodeStatus.ERRORED, str(e))
   ```

4. **Use Meaningful Names**: Make runs easy to identify
   ```python
   run = hl.Run(
       scenario=scenario,
       name=f"param_sweep_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
       description="Sweeping friction parameters",
       tags=["sweep", "friction", "v2"]
   )
   ```

5. **Tag Your Runs**: Use tags for organization and filtering
   ```python
   tags=["baseline", "physics", "experiment_1"]
   ```

## Complete Example

```python
import humalab as hl
from humalab.humalab_api_client import EpisodeStatus

# Initialize
hl.init(api_key="your_api_key")

# Create scenario
scenario = hl.scenarios.Scenario()
scenario.init(scenario={
    "learning_rate": "${log_uniform(0.0001, 0.1)}",
    "batch_size": "${discrete(3, [32, 64, 128], [0.3, 0.5, 0.2])}"
})

# Create run
run = hl.Run(
    scenario=scenario,
    project="hyperparameter_search",
    name="lr_batch_sweep",
    tags=["training", "optimization"]
)

# Execute episodes
with run:
    for i in range(50):
        with run.create_episode() as episode:
            # Train model with sampled parameters
            model = train_model(
                lr=episode.learning_rate,
                batch_size=episode.batch_size
            )

            # Log results
            episode.log({
                "accuracy": model.accuracy,
                "loss": model.loss
            })

# Cleanup
hl.finish()
```
